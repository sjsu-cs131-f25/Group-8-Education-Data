# -*- coding: utf-8 -*-
"""Project Assignment 6: Final Report.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xldUxbZxAujy9l7EZCIb89-eBFCRaIC_

#Project Assignment 6: Final Report

##Set Up PySpark:
"""

bucket_name = "cs131-group8-bucket"

data_path_1 = f"gs://{bucket_name}/data/student_performance.csv"

from pyspark.sql import SparkSession
spark = SparkSession.builder \
        .appName("Project_Assignment_6_Final_Report") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "4g") \
        .getOrCreate()
spark

"""##Import necessary libraries:"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
import statsmodels
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from sklearn.ensemble import RandomForestRegressor
from scipy.stats import pearsonr
from pyspark.ml.regression import LinearRegression, RandomForestRegressor
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.evaluation import RegressionEvaluator
from sklearn.preprocessing import OneHotEncoder
from google.cloud import storage
import io

# Initialize bucket
client = storage.Client()

bucket = client.bucket(bucket_name)
blob_name = "out/"
blob = bucket.blob(blob_name)

# Create a DataFrame reading from the CSV file
df = spark.read.csv(data_path_1, header=True, inferSchema=True)
df.show(20)

"""##Use OneHotEncoding to convert categorical data to numeric type:"""

# Create a OneHotEncoder to transform categorical columns into numerical columns in binary
categories = ['Resources', 'Extracurricular', 'Internet', 'Gender', 'LearningStyle', 'Discussions', 'EduTech', 'FinalGrade']
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

# Temporarily convert PySpark DataFrame to Pandas for easier one-hot encoding
dfp = df.toPandas()
encoded_dfp = encoder.fit_transform(dfp[categories])
encoded_dfp = pd.DataFrame(encoded_dfp, columns=encoder.get_feature_names_out(categories))

numerical_dfp = dfp.drop(columns=categories)
dfp = pd.concat([numerical_dfp, encoded_dfp], axis=1)

# Convert Pandas back into PySpark DataFrame
encoded_df = spark.createDataFrame(dfp)
encoded_df.show(20)

bucket.blob("out/enc_student_performance.csv").upload_from_string(dfp.to_csv(), "text/csv")
print("Table uploaded to out/")

# Create a VectorAssembler that puts all the independent variable columns into "features".
assembler = VectorAssembler(inputCols=[col for col in encoded_df.columns if col != 'ExamScore'], outputCol="features")
data = assembler.transform(encoded_df)
final_data = data.select('features', 'ExamScore')

final_data.show(20)
print(len(encoded_df.columns))

train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)

"""##Build linear regression and random forest models:"""

# Build a Linear Regression Model and a Random Forest Model to predict the RMSE and coefficient of determination
lr = LinearRegression(featuresCol='features', labelCol='ExamScore', predictionCol='predicted_score')
rf = RandomForestRegressor(featuresCol='features', labelCol='ExamScore', predictionCol='predicted_score',
                           numTrees=2, maxDepth=5)

lr_model = lr.fit(train_data)
rf_model = rf.fit(train_data)

train_predictions_lr = lr_model.transform(train_data)
test_predictions_lr = lr_model.transform(test_data)

train_predictions_rf = rf_model.transform(train_data)
test_predictions_rf = rf_model.transform(test_data)

evaluator = RegressionEvaluator(labelCol='ExamScore', predictionCol='predicted_score', metricName='rmse')
rmse = evaluator.evaluate(test_predictions_lr)
print("LR Root Mean Squared Error (RMSE): {:.3f}".format(rmse))

evaluator_r2 = RegressionEvaluator(labelCol='ExamScore', predictionCol='predicted_score', metricName='r2')
r2 = evaluator_r2.evaluate(test_predictions_lr)
print("LR R-squared (R2): {:.3f}".format(r2))

print()

rf_evaluator = RegressionEvaluator(labelCol='ExamScore', predictionCol='predicted_score', metricName='rmse')
rmse = rf_evaluator.evaluate(test_predictions_rf)
print("RF Root Mean Squared Error (RMSE): {:.3f}".format(rmse))

rf_evaluator_r2 = RegressionEvaluator(labelCol='ExamScore', predictionCol='predicted_score', metricName='r2')
r2 = rf_evaluator_r2.evaluate(test_predictions_rf)
print("RF R-squared (R2): {:.3f}".format(r2))

"""##Evaluate the features and create a bar plot:"""

# Evaluate each feature/column based on its importance to ExamScore, and then build a bar plot representation of the data.
feature_importance = sorted(list(zip(data.columns[:-1], map(abs, lr_model.coefficients))), key=lambda x: x[1], reverse=True)
print("Feature Importance:")
for feature, importance in feature_importance:
    print(f'{feature}: {importance:.3f}')

# Convert feature_importance list to Pandas DataFrame.
feature_importance_df = pd.DataFrame(feature_importance, columns=['Features', 'Importance'])

# Plot the feature importance on a bar plot.
feature_importance_df.plot.bar(x="Features", y="Importance", figsize=(15,8))

"""##Calculate the correlation matrix:"""

# Tranform dataset into pandas type
encoded_dfp = encoded_df.toPandas()

# Calculate the correlation matrix
corr_matrix = encoded_dfp.corr(numeric_only=True)

fig, ax = plt.subplots(figsize=(13,13))

# Create the heatmap
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"fontsize":7})

# Set the title
plt.title('Correlation Matrix')

buffer = io.BytesIO()
plt.savefig(buffer, format='png')
buffer.seek(0)
plt.close()

blob = bucket.blob("out/correlation_matrix.png")
blob.upload_from_string(buffer.getvalue(), content_type='image/png')

print("Correlation matrix uploaded to out/")

"""##Calculate VIF (Variance Inflation Factor):"""

# Before we can run the VIF matrix calculation, we need to normalize the data

from sklearn.preprocessing import MinMaxScaler

df = encoded_dfp.copy()
df.drop(['ExamScore', 'FinalGrade_A', 'FinalGrade_B', 'FinalGrade_C', 'FinalGrade_D'], axis=1, inplace=True)
columns_to_normalize = df.columns
scaler = MinMaxScaler()
encoded_dfp[columns_to_normalize] = scaler.fit_transform(encoded_dfp[columns_to_normalize])

# Import command to suppress warnings
import warnings
warnings.filterwarnings('ignore', category=RuntimeWarning)

X = encoded_dfp.select_dtypes(include=['number']).drop(columns=['ExamScore', 'FinalGrade_A', 'FinalGrade_B', 'FinalGrade_C', 'FinalGrade_D'], errors='ignore')

# Add a constant to the model (intercept)
X = add_constant(X)

for col in X.columns:
  # Check if the column contains infinite values
  if np.isinf(X[col]).any():
    # Replace infinite values with NaN
    X[col] = X[col].replace([np.inf, -np.inf], np.nan)
  if np.isnan(X[col]).any():
    # Impute NaN values with the column mean
    X[col] = X[col].fillna(X[col].mean())

vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns

# calculating VIF for each feature
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]

print(vif_data)

"""##Remove strongly correlated variables and create a Feature Importance Matrix:"""

# Removing all highly correlated variables

while vif_data['VIF'].max() > 5:
  row_index = vif_data['VIF'].idxmax()
  column_name = vif_data.loc[row_index, 'Feature']
  X = X.drop(column_name, axis=1)
  vif_data = pd.DataFrame()
  vif_data["Feature"] = X.columns
  vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]
print('\n\n')
print(vif_data)

from sklearn.ensemble import RandomForestRegressor

# Separate features (X) and target variable (y)
x = encoded_dfp[(selected_columns for selected_columns in vif_data['Feature'])].copy()
y = encoded_dfp['ExamScore']

# Train a Random Forest Regressor model
model = RandomForestRegressor()
model.fit(x, y)

# Get feature importances
importances = model.feature_importances_

# Create a DataFrame for feature importances
feature_importance_df = pd.DataFrame({'Feature': x.columns, 'Importance': importances})

# Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Print the feature importance matrix
print(feature_importance_df)

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Truly Independent Feature Importance Matrix')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

buffer = io.BytesIO()

plt.savefig(buffer, format='png')
buffer.seek(0)
plt.close()
blob = bucket.blob("out/feature_importance.png")
blob.upload_from_string(buffer.getvalue(), content_type='image/png')
print("Feature importance bar graph uploaded to out/")

"""##Calculate relative importance of independent variables:"""

# Define variables for scatter plots (PURE Pandas)
x0 = encoded_dfp['Attendance']
y0 = encoded_dfp['ExamScore']

x1 = encoded_dfp['AssignmentCompletion']
y1 = encoded_dfp['ExamScore']

x2 = encoded_dfp['OnlineCourses']
y2 = encoded_dfp['ExamScore']

# Create subplots
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Calculate correlation coefficients
r1_value, p1_value = pearsonr(x0, y0)
r2_value, p2_value = pearsonr(x1, y1)
r3_value, p3_value = pearsonr(x2, y2)

# Plot 1
axes[0].scatter(x0, y0, color='blue', s=4)
axes[0].set_title(f"Attendance vs ExamScore\nR = {r1_value:.2f}")
axes[0].set_xlabel('Attendance')
axes[0].set_ylabel('ExamScore')

# Plot 2
axes[1].scatter(x1, y1, color='purple', s=4)
axes[1].set_title(f"AssignmentCompletion vs ExamScore\nR = {r2_value:.2f}")
axes[1].set_xlabel('AssignmentCompletion')
axes[1].set_ylabel('ExamScore')

# Plot 3
axes[2].scatter(x2, y2, color='orange', s=4)
axes[2].set_title(f"OnlineCourses vs ExamScore\nR = {r3_value:.2f}")
axes[2].set_xlabel('OnlineCourses')
axes[2].set_ylabel('ExamScore')

plt.tight_layout()
buffer = io.BytesIO()

plt.savefig(buffer, format='png')
buffer.seek(0)
plt.close()
blob = bucket.blob("out/scatter_plots.png")
blob.upload_from_string(buffer.getvalue(), content_type='image/png')

print("Scatter plots uploaded to out/")
